# Tantra â€” CPUâ€‘first Voice+Text LLM Agent (Flat, Fast, Friendly)

Tantra is a minimal, GPUâ€‘optional assistant that runs anywhere (Windows/Linux/macOS). It speaks, listens, plans, uses tools, and remembersâ€”without complex setup.

## âœ¨ Features
- Humanâ€‘like chat (voice + text), bargeâ€‘in friendly
- LLMâ€‘first planning with safe tool use (web, files, calc, shell)
- Memory: shortâ€‘term context + longâ€‘term vector memory (FAISS)
- RAGâ€‘lite: build local indexes and cite sources
- CPUâ€‘first runtime; GPU optional
- Flat repo, configâ€‘driven, no hardcoded prompts

## ğŸš€ Quickstart
1) Python 3.10+
2) Install deps:
```bash
pip install -r requirements.txt
```
3) Start REST API:
```bash
python Training/serve_api.py
```
4) Start realtime WS (voice):
```bash
uvicorn Training.serve_realtime:app --host 0.0.0.0 --port 8001
```

## ğŸ“¦ Model Assets
- Tokenizer: `Model/tokenizer.json`
- Vocabulary: `Model/vocab.json` (optional)
- Weights: `Model/tantra_weights.safetensors` (+ `Model/tantra_weights.bak`)

### Build tokenizer (example)
```bash
python Training/tokenizer_train.py --input_glob "Dataset/*.jsonl" --out Model/tokenizer.json
```
A tiny sample is provided at `Dataset/sample.jsonl`. Use your real data for meaningful tokenization.

### Weights
- Place your checkpoint at `Model/tantra_weights.safetensors`
- Update `Config/serve.yaml` paths if needed

## âš™ï¸ Configs
- `Config/serve.yaml` â€” CPU, quant, telemetry
- `Config/agent.yaml` â€” persona, planning, tools, memory, safety
- `Config/realtime.yaml` â€” VAD mode, bargeâ€‘in, wakeâ€‘word

## ğŸ§  Architecture (highâ€‘level)
- `Training/model_runtime.py` â€” CPU text gen via transformers
- `Training/agent.py` â€” ReAct planner + tool registry + safety
- `Training/memory.py` â€” convo buffer + summarizer hooks
- `Training/embedding_build.py` â€” MiniLM embeddings + FAISS HNSW
- `Training/rag_index.py` â€” index I/O
- `Training/serve_api.py` â€” FastAPI REST (`/infer`, `/agent/stream`)
- `Training/serve_realtime.py` â€” WebSocket voice loop (`/voice`)

## ğŸ” RAG Build
```bash
python Training/embedding_build.py --input_files docs.txt notes.md --out Model/
```
This creates `Model/rag_texts.json` and `Model/rag_index.faiss`.

## ğŸ”Š Speech
- STT: Whisper (optional; CPU works, GPU optional)
- TTS: pyttsx3 (no network) or Piper (optional)

## ğŸ—ºï¸ Roadmap
- v1: CPUâ€‘first voice+text, tools, memory, RAGâ€‘lite, safety, eval
- v1.1: wakeâ€‘word, metrics dashboard, richer tools
- v2: enable vision head (`Training/model_multimodal.py`) and `/vision/infer`

## ğŸ“š Docs
See `Docs/DOCS.md` for endpoints and examples.

## ğŸ“ License
MIT Â© 2025 Atulya AI and Contributors

## ğŸ”— Links
- Repo: `https://github.com/atulyaai/Tantra-LLM`
